{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright @ cb_park@korea.ac.kr (Cheonbok Park), joonleesky@kaist.ac.kr (Hojoon Lee)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Feed-Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn #\n",
    "import torch.nn.functional as F # various activation functions for model\n",
    "import torchvision # You can load various Pretrained Model from this package \n",
    "import torchvision.datasets as vision_dsets\n",
    "import torchvision.transforms as T # Transformation functions to manipulate images\n",
    "import torch.optim as optim # various optimization functions for model\n",
    "from torch.autograd import Variable \n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initilaize Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MNIST_DATA(root='./data',train =True,transforms=None ,download =True,batch_size = 32,num_worker = 1):\n",
    "    print (\"[+] Get the MNIST DATA\")\n",
    "    \"\"\"\n",
    "    We will use Mnist data for our tutorial \n",
    "    \"\"\"\n",
    "    mnist_train = vision_dsets.MNIST(root = root,  #root is the place to store your data. \n",
    "                                    train = True,  \n",
    "                                    transform = T.ToTensor(), # convert data to tensor \n",
    "                                    download = True)  # whether to download the data\n",
    "    mnist_test = vision_dsets.MNIST(root = root,\n",
    "                                    train = False, \n",
    "                                    transform = T.ToTensor(),\n",
    "                                    download = True)\n",
    "    \"\"\"\n",
    "    Data Loader is a iterator that fetches the data with the number of desired batch size. \n",
    "    * Practical Guide : What is the optimal batch size? \n",
    "      - Usually.., higher the batter. \n",
    "      - We recommend to use it as a multiple of 2 to efficiently utilize the gpu memory. (related to bit size)\n",
    "    \"\"\"\n",
    "    trainDataLoader = data.DataLoader(dataset = mnist_train,  # information about your data type\n",
    "                                      batch_size = batch_size, # batch size\n",
    "                                      shuffle =True, # Whether to shuffle your data for every epoch. (Very important for training performance)\n",
    "                                      num_workers = 1) # number of workers to load your data. (usually number of cpu cores)\n",
    "\n",
    "    testDataLoader = data.DataLoader(dataset = mnist_test, \n",
    "                                    batch_size = batch_size,\n",
    "                                    shuffle = False, # we don't actually need to shuffle data for test\n",
    "                                    num_workers = 1) #\n",
    "    print (\"[+] Finished loading data & Preprocessing\")\n",
    "    return mnist_train,mnist_test,trainDataLoader,testDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Get the MNIST DATA\n",
      "[+] Finished loading data & Preprocessing\n"
     ]
    }
   ],
   "source": [
    "trainDset,testDset,trainDataLoader,testDataLoader= MNIST_DATA(batch_size = 32)  # Data Loader "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, trainloader, testloader, net, optimizer, criterion):\n",
    "        \"\"\"\n",
    "        trainloader: train data's loader\n",
    "        testloader: test data's loader\n",
    "        net: model to train\n",
    "        optimizer: optimizer to update your model\n",
    "        criterion: loss function\n",
    "        \"\"\"\n",
    "        self.trainloader = trainloader\n",
    "        self.testloader = testloader\n",
    "        self.net = net\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        \n",
    "    def train(self, epoch = 1):\n",
    "        \"\"\"\n",
    "        epoch: number of times each training sample is used\n",
    "        \"\"\"\n",
    "        self.net.train()\n",
    "        for e in range(epoch):\n",
    "            running_loss = 0.0  \n",
    "            for i, data in enumerate(self.trainloader, 0): \n",
    "                # get the inputs\n",
    "                inputs, labels = data # Return type for data in dataloader is tuple of (input_data, labels)\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "                # zero the parameter gradients\n",
    "                self.optimizer.zero_grad()    \n",
    "                #  Q1) what if we dind't clear up the gradients?\n",
    "                #  A1) Whenever .background() is called, Pytorch accumulates the gradients in buffers on subsequent backward pass.\n",
    "                #      If we didn't clear up them, it can be mixed with the previous gradients, and values can diverge\n",
    "                #      when the number of iterations increases. So, if you don't want to mix up gradients between minibatches,\n",
    "                #      you have to zero them out at the start of a new minibatch.\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = self.net(inputs) # get output after passing through the network\n",
    "                loss = self.criterion(outputs, labels) # compute model's score using the loss function \n",
    "                loss.backward() # perform back-propagation from the loss\n",
    "                self.optimizer.step() # perform gradient descent with given optimizer\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "                if (i+1) % 500 == 0:    # print every 2000 mini-batches\n",
    "                    print('[%d, %5d] loss: %.3f' % (e + 1, i + 1, running_loss / 500))\n",
    "                    running_loss = 0.0\n",
    "\n",
    "        print('Finished Training')\n",
    "        \n",
    "    def test(self):\n",
    "        self.net.eval() # Q2) Why should we change the network into eval-mode?\n",
    "                        # A2) model.eval() sets the module in evaluation mode.\n",
    "                        #     It is a kind of switch for some specific layers/parts (e.g. Dropout, BatchNorm) of the model\n",
    "                        #     that behave differently during training and inference time.\n",
    "                        #     For example, in eval mode the Dropout is deactivated and BatchNorm uses the parameters saved in training.\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        \n",
    "        # Data for confusion matrix\n",
    "        confusion_matrix = torch.zeros(10, 10)\n",
    "        for inputs, labels in self.testloader:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda() \n",
    "            output = self.net(inputs) \n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max \n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "            test_loss /= len(self.testloader.dataset)\n",
    "            \n",
    "            # Compute confusion matrix\n",
    "            for t, p in zip(labels.view(-1).cpu().numpy(), pred.view(-1).cpu().numpy()):\n",
    "                print('work!')\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "                \n",
    "        print('\\nTest set:  Accuracy: {}/{} ({:.0f}%)\\n'.\n",
    "                format(correct, len(self.testloader.dataset),\n",
    "                100.* correct / len(self.testloader.dataset)))\n",
    "        print('\\nConfusion matrix\\n')\n",
    "        print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create Model by yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![activation](./imgs/activation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 2-Layer Network + Sigmoid\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Hidden dimension: 30\n",
    "- Output dimension: 10\n",
    "- activation: sigmoid\n",
    "- Optimizer: SGD\n",
    "- Loss: Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__() # Q3) Why do we need to call the constructor, nn.Module?\n",
    "                                          # A3) \n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc0 = nn.Linear(28*28,30)\n",
    "        self.fc1 = nn.Linear(30, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,28*28) # x.view convert the shape of tensor, (Batch_size,28,28) --> (Batch_size,28*28)\n",
    "        x = self.fc0(x) # 28*28 -> 30 \n",
    "        x = F.sigmoid(x) # Activation function \n",
    "        x = self.fc1(x)  # 30 -> 10, logit for each class\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() # create the neural network instance and load to the cuda memory.\n",
    "criterion = nn.CrossEntropyLoss() # Define Loss Function. We use Cross-Entropy loss.\n",
    "optimizer = optim.SGD(mnist_net.parameters(), lr=0.001) # optimizer receives training parameters and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(trainloader = trainDataLoader,\n",
    "                  testloader = testDataLoader,\n",
    "                  net = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 2-Layer Network + ReLU\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Hidden dimension: 30\n",
    "- Output dimension: 10\n",
    "- activation: relu\n",
    "- Optimizer: SGD\n",
    "- Loss: Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__() \n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc0 = nn.Linear(28*28,30)\n",
    "        self.fc1 = nn.Linear(30, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,28*28) \n",
    "        x = self.fc0(x) # 28*28 -> 30 \n",
    "        x = F.relu(x) # Activation function\n",
    "        x = self.fc1(x)  # 30 -> 10\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.SGD(mnist_net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(trainloader = trainDataLoader,\n",
    "                  testloader = testDataLoader,\n",
    "                  net = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4) Is there any difference in performance according to the activiation function?\n",
    "\n",
    "#### Ans) Yes. When comparing inference of ReLU and Sigmoid in the same Network, the accuracy of ReLU is about 85%, which is better than 59% of Sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 3-Layer Network + Sigmoid\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Hidden dimension: (50, 30)\n",
    "- Output dimension: 10\n",
    "- activation: sigmoid\n",
    "- Optimizer: SGD\n",
    "- Loss: Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        self.fc0 = nn.Linear(28*28,50) # Layer 1\n",
    "        self.fc1 = nn.Linear(50, 30) # Layer 2\n",
    "        self.fc2 = nn.Linear(30, 10) # Layer 3\n",
    "\n",
    "    def forward(self, x):\n",
    "      \n",
    "        x = x.view(-1,28*28)\n",
    "        x = self.fc0(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.SGD(mnist_net.parameters(), lr=0.001)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(trainloader = trainDataLoader,\n",
    "                  testloader = testDataLoader,\n",
    "                  net = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5) Is training gets done easily? If it doesn't, why not?\n",
    "\n",
    "#### Ans) No."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) 3-Layer Network + ReLU\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Hidden dimension: (50, 30)\n",
    "- Output dimension: 10\n",
    "- activation: relu\n",
    "- Optimizer: SGD\n",
    "- Loss: Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        self.fc0 = nn.Linear(28*28,50) # Layer 1\n",
    "        self.fc1 = nn.Linear(50, 30) # Layer 2\n",
    "        self.fc2 = nn.Linear(30, 10) # Layer 3\n",
    "\n",
    "    def forward(self, x):\n",
    "      \n",
    "        x = x.view(-1,28*28)\n",
    "        x = self.fc0(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.SGD(mnist_net.parameters(), lr=0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(trainloader = trainDataLoader,\n",
    "                  testloader = testDataLoader,\n",
    "                  net = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6) Is training gets done easily compared to experiment (2)? If it doesn't, why not?\n",
    "\n",
    "#### Ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q7) What would happen if there is no activation function?\n",
    "\n",
    "#### Ans) If there is no activation function, the output signal would simply be a single linear function. Linear equation is easy to solve but it is limited in its complexity and hard to learn complex functional mapping from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Change our Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Adam](./imgs/adam.jpeg)\n",
    "\n",
    "Reference: 하용호, 자습해도 모르겠던 딥러닝, 머리속에 인스톨 시켜드립니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) 3-Layer Network + ReLU + Adam\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Hidden dimension: (50, 30)\n",
    "- Output dimension: 10\n",
    "- activation: relu\n",
    "- Optimizer: Adam\n",
    "- Loss: Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        self.fc0 = nn.Linear(28*28,50) # Layer 1\n",
    "        self.fc1 = nn.Linear(50, 30) # Layer 2\n",
    "        self.fc2 = nn.Linear(30, 10) # Layer 3\n",
    "\n",
    "    def forward(self, x):\n",
    "      \n",
    "        x = x.view(-1,28*28)\n",
    "        x = self.fc0(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() \n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(trainloader = trainDataLoader,\n",
    "                  testloader = testDataLoader,\n",
    "                  net = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) 2-Layer Network + ReLU + Adam\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Hidden dimension: (30)\n",
    "- Output dimension: 10\n",
    "- activation: relu\n",
    "- Optimizer: Adam\n",
    "- Loss: Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__() \n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc0 = nn.Linear(28*28,30)\n",
    "        self.fc1 = nn.Linear(30, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,28*28) \n",
    "        x = self.fc0(x) # 28*28 -> 30 \n",
    "        x = F.relu(x) \n",
    "        x = self.fc1(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(trainloader = trainDataLoader,\n",
    "                  testloader = testDataLoader,\n",
    "                  net = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch-Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![normalization](./imgs/normalization.png)\n",
    "\n",
    "Reference: Andrew Ng, Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7) 2-Layer Network + ReLU + Adam + Batch-Norm\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Hidden dimension: (30)\n",
    "- Output dimension: 10\n",
    "- activation: relu\n",
    "- normalization: batch-norm\n",
    "- Optimizer: Adam\n",
    "- Loss: Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__() \n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc0 = nn.Linear(28*28,30)\n",
    "        self.bn0 = nn.BatchNorm1d(30) # BatchNorm \n",
    "        self.fc1 = nn.Linear(30, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,28*28) \n",
    "        x = self.fc0(x) # 28*28 -> 30 \n",
    "        x = self.bn0(x)\n",
    "        x = F.relu(x) \n",
    "        x = self.fc1(x)   \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(trainloader = trainDataLoader,\n",
    "                  testloader = testDataLoader,\n",
    "                  net = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (8) 3-Layer Network + ReLU + Adam + Batch-Norm\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Hidden dimension: (50, 30)\n",
    "- Output dimension: 10\n",
    "- activation: relu\n",
    "- normalization: batch-norm\n",
    "- Optimizer: Adam\n",
    "- Loss: Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        self.fc0 = nn.Linear(28*28,50) # Layer 1\n",
    "        self.bn0 = nn.BatchNorm1d(50) # BatchNorm 1 \n",
    "        self.fc1 = nn.Linear(50, 30) # Layer 2\n",
    "        self.bn1 = nn.BatchNorm1d(30) # BatchNorm 2\n",
    "        self.fc2 = nn.Linear(30, 10) # Layer 3\n",
    "\n",
    "    def forward(self, x):\n",
    "      \n",
    "        x = x.view(-1,28*28)\n",
    "        x = self.fc0(x)\n",
    "        x = self.bn0(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(trainloader = trainDataLoader,\n",
    "                  testloader = testDataLoader,\n",
    "                  net = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q8) Is there any performance difference before/after applying the batch-norm?\n",
    "\n",
    "#### Ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q9) How did 2-layer neural network and 3-layer neural network behave differently after applying the batch-nrom?\n",
    "\n",
    "#### Ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 1.1 Let's Do It: Let's achieve performance greater than 98%!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        self.conv0 = nn.Conv2d(1, 8, 3, 1, 1)\n",
    "        self.conv0_bn = nn.BatchNorm2d(8)\n",
    "        self.pool0 = nn.MaxPool2d(2)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(8, 16, 3, 1, 1)\n",
    "        self.conv1_bn = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.fc = nn.Linear(16*7*7, 256)\n",
    "        self.fc1 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv0(x)\n",
    "        x = self.conv0_bn(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool0(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv1_bn(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
    "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
    "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   500] loss: 0.412\n",
      "[1,  1000] loss: 0.164\n",
      "[1,  1500] loss: 0.136\n",
      "[2,   500] loss: 0.102\n",
      "[2,  1000] loss: 0.092\n",
      "[2,  1500] loss: 0.087\n",
      "[3,   500] loss: 0.073\n",
      "[3,  1000] loss: 0.066\n",
      "[3,  1500] loss: 0.072\n",
      "[4,   500] loss: 0.059\n",
      "[4,  1000] loss: 0.065\n",
      "[4,  1500] loss: 0.061\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = trainDataLoader,\n",
    "                  testloader = testDataLoader,\n",
    "                  net = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)\n",
    "\n",
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set:  Accuracy: 9876/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204826"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(mnist_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q10) What are the problems with the large number of parameters?\n",
    "\n",
    "#### Ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's use Convolution Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Convolution](./imgs/Conv.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q11) Given input image with shape:(H, W, C1), what would be the shape of output image after applying 2 (F * F) convolutional filters with stride S?\n",
    "#### Ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (9) 2-Layer Network (Conv+Fc) + ReLU + Adam + Batch-Norm\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Conv: 8 (6 * 6) filter with stride=2 \n",
    "- Hidden dimension: 8 * 12 * 12\n",
    "- Output dimension: 10\n",
    "- activation: relu\n",
    "- normalization: batch-norm\n",
    "- Optimizer: Adam\n",
    "- Loss: Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        self.conv0 = nn.Conv2d(in_channels = 1,\n",
    "                               out_channels = 8,\n",
    "                               kernel_size = 6,\n",
    "                               stride = 2) # Layer 1\n",
    "        self.conv0_bn = nn.BatchNorm2d(8)  # 2d batch-norm is used in 3d inputs\n",
    "        self.fc = nn.Linear(8*12*12, 10)   # Layer 2 \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv0(x)\n",
    "        x = self.conv0_bn(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(trainloader = trainDataLoader,\n",
    "                  testloader = testDataLoader,\n",
    "                  net = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(mnist_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q12) How did the performance and the number of parameters change after using the Convolution operation? Why did these results come out?\n",
    "\n",
    "#### Ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (10) 2-Layer Network (Conv+Pool+Fc) + ReLU + Adam + Batch-Norm\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Conv: 8 (7 * 7) filter with stride=2 \n",
    "- Pool: 2 * 2\n",
    "- Hidden dimension: 8 * 6 * 6\n",
    "- Output dimension: 10\n",
    "- activation: relu\n",
    "- normalization: batch-norm\n",
    "- Optimizer: Adam\n",
    "- Loss: Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pooling](./imgs/Pool.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        self.conv0 = nn.Conv2d(in_channels = 1,\n",
    "                               out_channels = 8,\n",
    "                               kernel_size = 6,\n",
    "                               stride = 2) # Layer 1\n",
    "        self.conv0_bn = nn.BatchNorm2d(8)  \n",
    "        self.pool0 = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(8*6*6, 10) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv0(x)\n",
    "        x = self.conv0_bn(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool0(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() \n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(trainloader = trainDataLoader,\n",
    "                  testloader = testDataLoader,\n",
    "                  net = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(mnist_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q13) How did the performance change after using the Convolution operation? Why did these results come out?\n",
    "\n",
    "#### Ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
