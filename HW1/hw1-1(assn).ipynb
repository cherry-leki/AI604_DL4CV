{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright @ cb_park@korea.ac.kr (Cheonbok Park), joonleesky@kaist.ac.kr (Hojoon Lee)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Feed-Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn #\n",
    "import torch.nn.functional as F # various activation functions for model\n",
    "import torchvision # You can load various Pretrained Model from this package \n",
    "import torchvision.datasets as vision_dsets\n",
    "import torchvision.transforms as T # Transformation functions to manipulate images\n",
    "import torch.optim as optim # various optimization functions for model\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable \n",
    "from torch.utils import data\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initilaize Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MNIST_DATA(root='./data',train =True,transforms=None ,download =True,batch_size = 32,num_worker = 1):\n",
    "    print (\"[+] Get the MNIST DATA\")\n",
    "    \"\"\"\n",
    "    We will use Mnist data for our tutorial \n",
    "    \"\"\"\n",
    "    mnist_train = vision_dsets.MNIST(root = root,  #root is the place to store your data. \n",
    "                                    train = True,  \n",
    "                                    transform = T.ToTensor(), # convert data to tensor \n",
    "                                    download = True)  # whether to download the data\n",
    "    mnist_test = vision_dsets.MNIST(root = root,\n",
    "                                    train = False, \n",
    "                                    transform = T.ToTensor(),\n",
    "                                    download = True)\n",
    "    \"\"\"\n",
    "    Data Loader is a iterator that fetches the data with the number of desired batch size. \n",
    "    * Practical Guide : What is the optimal batch size? \n",
    "      - Usually.., higher the batter. \n",
    "      - We recommend to use it as a multiple of 2 to efficiently utilize the gpu memory. (related to bit size)\n",
    "    \"\"\"\n",
    "    trainDataLoader = data.DataLoader(dataset = mnist_train,  # information about your data type\n",
    "                                      batch_size = batch_size, # batch size\n",
    "                                      shuffle =True, # Whether to shuffle your data for every epoch. (Very important for training performance)\n",
    "                                      num_workers = 1) # number of workers to load your data. (usually number of cpu cores)\n",
    "\n",
    "    testDataLoader = data.DataLoader(dataset = mnist_test, \n",
    "                                    batch_size = batch_size,\n",
    "                                    shuffle = False, # we don't actually need to shuffle data for test\n",
    "                                    num_workers = 1) #\n",
    "    print (\"[+] Finished loading data & Preprocessing\")\n",
    "    return mnist_train,mnist_test,trainDataLoader,testDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Get the MNIST DATA\n",
      "[+] Finished loading data & Preprocessing\n"
     ]
    }
   ],
   "source": [
    "trainDset,testDset,trainDataLoader,testDataLoader= MNIST_DATA(batch_size = 32)  # Data Loader "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, trainloader, testloader, net, optimizer, criterion):\n",
    "        \"\"\"\n",
    "        trainloader: train data's loader\n",
    "        testloader: test data's loader\n",
    "        net: model to train\n",
    "        optimizer: optimizer to update your model\n",
    "        criterion: loss function\n",
    "        \"\"\"\n",
    "        self.trainloader = trainloader\n",
    "        self.testloader = testloader\n",
    "        self.net = net\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        \n",
    "    def train(self, epoch = 1):\n",
    "        \"\"\"\n",
    "        epoch: number of times each training sample is used\n",
    "        \"\"\"\n",
    "        self.net.train()\n",
    "        for e in range(epoch):\n",
    "            running_loss = 0.0  \n",
    "            for i, data in enumerate(self.trainloader, 0): \n",
    "                # get the inputs\n",
    "                inputs, labels = data # Return type for data in dataloader is tuple of (input_data, labels)\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "                # zero the parameter gradients\n",
    "                self.optimizer.zero_grad()    \n",
    "                #  Q1) what if we dind't clear up the gradients?\n",
    "                #  A1) Whenever .background() is called, Pytorch accumulates the gradients in buffers on subsequent backward pass.\n",
    "                #      If we didn't clear up them, it can be mixed with the previous gradients, and values can diverge\n",
    "                #      when the number of iterations increases. So, if you don't want to mix up gradients between minibatches,\n",
    "                #      you have to zero them out at the start of a new minibatch.\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = self.net(inputs) # get output after passing through the network\n",
    "                loss = self.criterion(outputs, labels) # compute model's score using the loss function \n",
    "                loss.backward() # perform back-propagation from the loss\n",
    "                self.optimizer.step() # perform gradient descent with given optimizer\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "                if (i+1) % 500 == 0:    # print every 2000 mini-batches\n",
    "                    print('[%d, %5d] loss: %.3f' % (e + 1, i + 1, running_loss / 500))\n",
    "                    running_loss = 0.0\n",
    "\n",
    "        print('Finished Training')\n",
    "        \n",
    "    def test(self):\n",
    "        self.net.eval() # Q2) Why should we change the network into eval-mode?\n",
    "                        # A2) model.eval() sets the module in evaluation mode.\n",
    "                        #     It is a kind of switch for some specific layers/parts (e.g. Dropout, BatchNorm) of the model\n",
    "                        #     that behave differently during training and inference time.\n",
    "                        #     For example, in eval mode the Dropout is deactivated and BatchNorm uses the parameters saved in training.\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        \n",
    "        # Data for confusion matrix\n",
    "        for inputs, labels in self.testloader:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda() \n",
    "            output = self.net(inputs) \n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max \n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "            test_loss /= len(self.testloader.dataset)\n",
    "            \n",
    "                \n",
    "        print('\\nTest set:  Accuracy: {}/{} ({:.0f}%)\\n'.\n",
    "                format(correct, len(self.testloader.dataset),\n",
    "                100.* correct / len(self.testloader.dataset)))\n",
    "        \n",
    "    def compute_conf(self):\n",
    "        self.net.eval()\n",
    "        \n",
    "        # Data for confusion matrix\n",
    "        conf_true = torch.zeros(0, dtype=torch.long, device='cpu')\n",
    "        conf_pred = torch.zeros(0, dtype=torch.long, device='cpu')\n",
    "\n",
    "        for inputs, labels in self.testloader:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda() \n",
    "            output = self.net(inputs) \n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max \n",
    "            \n",
    "            # Append batch prediction results for confusion matrix\n",
    "            conf_true = torch.cat([conf_true, labels.view(-1).cpu()])\n",
    "            conf_pred = torch.cat([conf_pred, pred.view(-1).cpu()])\n",
    "\n",
    "        # Print confusion matrix\n",
    "        label_name = [str(i) for i in range(10)]\n",
    "        conf_mat = confusion_matrix(conf_true.numpy(), conf_pred.numpy(), labels=[i for i in range(10)])\n",
    "        print('\\nConfusion matrix\\n')\n",
    "        print(conf_mat)\n",
    "\n",
    "        # per-class accuracy\n",
    "        conf_class_acc = 100 * conf_mat.diagonal()/conf_mat.sum(1)\n",
    "        print('\\nAccuracy per class\\n')\n",
    "        for i in range(10):\n",
    "            print(i, 'th acc: ', conf_class_acc[i])\n",
    "\n",
    "        # confusion matrix figure\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        for i in range(10):\n",
    "            for j in range(10):\n",
    "                texts = ax.text(j, i, conf_mat[i, j], ha=\"center\", va=\"center\", color=\"w\")\n",
    "        cax = ax.matshow(conf_mat)\n",
    "        fig.colorbar(cax)\n",
    "        ax.set_xticks([i for i in range(10)])\n",
    "        ax.set_yticks([i for i in range(10)])\n",
    "        ax.set_xticklabels(label_name)\n",
    "        ax.set_yticklabels(label_name)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create Model by yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![activation](./imgs/activation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 2-Layer Network + Sigmoid\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Hidden dimension: 30\n",
    "- Output dimension: 10\n",
    "- activation: sigmoid\n",
    "- Optimizer: SGD\n",
    "- Loss: Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__() # Q3) Why do we need to call the constructor, nn.Module?\n",
    "                                          # A3) \n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc0 = nn.Linear(28*28,30)\n",
    "        self.fc1 = nn.Linear(30, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,28*28) # x.view convert the shape of tensor, (Batch_size,28,28) --> (Batch_size,28*28)\n",
    "        x = self.fc0(x) # 28*28 -> 30 \n",
    "        x = F.sigmoid(x) # Activation function \n",
    "        x = self.fc1(x)  # 30 -> 10, logit for each class\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() # create the neural network instance and load to the cuda memory.\n",
    "criterion = nn.CrossEntropyLoss() # Define Loss Function. We use Cross-Entropy loss.\n",
    "optimizer = optim.SGD(mnist_net.parameters(), lr=0.001) # optimizer receives training parameters and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(trainloader = trainDataLoader,\n",
    "                  testloader = testDataLoader,\n",
    "                  net = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 2-Layer Network + ReLU\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Hidden dimension: 30\n",
    "- Output dimension: 10\n",
    "- activation: relu\n",
    "- Optimizer: SGD\n",
    "- Loss: Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__() \n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc0 = nn.Linear(28*28,30)\n",
    "        self.fc1 = nn.Linear(30, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,28*28) \n",
    "        x = self.fc0(x) # 28*28 -> 30 \n",
    "        x = F.relu(x) # Activation function\n",
    "        x = self.fc1(x)  # 30 -> 10\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.SGD(mnist_net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(trainloader = trainDataLoader,\n",
    "                  testloader = testDataLoader,\n",
    "                  net = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4) Is there any difference in performance according to the activiation function?\n",
    "\n",
    "#### Ans) Yes. When comparing inference of ReLU and Sigmoid in the same Network, the accuracy of ReLU is about 85%, which is better than 59% of Sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 3-Layer Network + Sigmoid\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Hidden dimension: (50, 30)\n",
    "- Output dimension: 10\n",
    "- activation: sigmoid\n",
    "- Optimizer: SGD\n",
    "- Loss: Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        self.fc0 = nn.Linear(28*28,50) # Layer 1\n",
    "        self.fc1 = nn.Linear(50, 30) # Layer 2\n",
    "        self.fc2 = nn.Linear(30, 10) # Layer 3\n",
    "\n",
    "    def forward(self, x):\n",
    "      \n",
    "        x = x.view(-1,28*28)\n",
    "        x = self.fc0(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.SGD(mnist_net.parameters(), lr=0.001)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(trainloader = trainDataLoader,\n",
    "                  testloader = testDataLoader,\n",
    "                  net = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5) Is training gets done easily? If it doesn't, why not?\n",
    "\n",
    "#### Ans) No."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) 3-Layer Network + ReLU\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Hidden dimension: (50, 30)\n",
    "- Output dimension: 10\n",
    "- activation: relu\n",
    "- Optimizer: SGD\n",
    "- Loss: Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        self.fc0 = nn.Linear(28*28,50) # Layer 1\n",
    "        self.fc1 = nn.Linear(50, 30) # Layer 2\n",
    "        self.fc2 = nn.Linear(30, 10) # Layer 3\n",
    "\n",
    "    def forward(self, x):\n",
    "      \n",
    "        x = x.view(-1,28*28)\n",
    "        x = self.fc0(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.SGD(mnist_net.parameters(), lr=0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(trainloader = trainDataLoader,\n",
    "                  testloader = testDataLoader,\n",
    "                  net = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6) Is training gets done easily compared to experiment (2)? If it doesn't, why not?\n",
    "\n",
    "#### Ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q7) What would happen if there is no activation function?\n",
    "\n",
    "#### Ans) If there is no activation function, the output signal would simply be a single linear function. Linear equation is easy to solve but it is limited in its complexity and hard to learn complex functional mapping from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Change our Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Adam](./imgs/adam.jpeg)\n",
    "\n",
    "Reference: 하용호, 자습해도 모르겠던 딥러닝, 머리속에 인스톨 시켜드립니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) 3-Layer Network + ReLU + Adam\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Hidden dimension: (50, 30)\n",
    "- Output dimension: 10\n",
    "- activation: relu\n",
    "- Optimizer: Adam\n",
    "- Loss: Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        self.fc0 = nn.Linear(28*28,50) # Layer 1\n",
    "        self.fc1 = nn.Linear(50, 30) # Layer 2\n",
    "        self.fc2 = nn.Linear(30, 10) # Layer 3\n",
    "\n",
    "    def forward(self, x):\n",
    "      \n",
    "        x = x.view(-1,28*28)\n",
    "        x = self.fc0(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() \n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(trainloader = trainDataLoader,\n",
    "                  testloader = testDataLoader,\n",
    "                  net = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) 2-Layer Network + ReLU + Adam\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Hidden dimension: (30)\n",
    "- Output dimension: 10\n",
    "- activation: relu\n",
    "- Optimizer: Adam\n",
    "- Loss: Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__() \n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc0 = nn.Linear(28*28,30)\n",
    "        self.fc1 = nn.Linear(30, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,28*28) \n",
    "        x = self.fc0(x) # 28*28 -> 30 \n",
    "        x = F.relu(x) \n",
    "        x = self.fc1(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(trainloader = trainDataLoader,\n",
    "                  testloader = testDataLoader,\n",
    "                  net = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch-Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![normalization](./imgs/normalization.png)\n",
    "\n",
    "Reference: Andrew Ng, Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7) 2-Layer Network + ReLU + Adam + Batch-Norm\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Hidden dimension: (30)\n",
    "- Output dimension: 10\n",
    "- activation: relu\n",
    "- normalization: batch-norm\n",
    "- Optimizer: Adam\n",
    "- Loss: Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__() \n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc0 = nn.Linear(28*28,30)\n",
    "        self.bn0 = nn.BatchNorm1d(30) # BatchNorm \n",
    "        self.fc1 = nn.Linear(30, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,28*28) \n",
    "        x = self.fc0(x) # 28*28 -> 30 \n",
    "        x = self.bn0(x)\n",
    "        x = F.relu(x) \n",
    "        x = self.fc1(x)   \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(trainloader = trainDataLoader,\n",
    "                  testloader = testDataLoader,\n",
    "                  net = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (8) 3-Layer Network + ReLU + Adam + Batch-Norm\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Hidden dimension: (50, 30)\n",
    "- Output dimension: 10\n",
    "- activation: relu\n",
    "- normalization: batch-norm\n",
    "- Optimizer: Adam\n",
    "- Loss: Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        self.fc0 = nn.Linear(28*28,50) # Layer 1\n",
    "        self.bn0 = nn.BatchNorm1d(50) # BatchNorm 1 \n",
    "        self.fc1 = nn.Linear(50, 30) # Layer 2\n",
    "        self.bn1 = nn.BatchNorm1d(30) # BatchNorm 2\n",
    "        self.fc2 = nn.Linear(30, 10) # Layer 3\n",
    "\n",
    "    def forward(self, x):\n",
    "      \n",
    "        x = x.view(-1,28*28)\n",
    "        x = self.fc0(x)\n",
    "        x = self.bn0(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(trainloader = trainDataLoader,\n",
    "                  testloader = testDataLoader,\n",
    "                  net = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q8) Is there any performance difference before/after applying the batch-norm?\n",
    "\n",
    "#### Ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q9) How did 2-layer neural network and 3-layer neural network behave differently after applying the batch-nrom?\n",
    "\n",
    "#### Ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 1.1 Let's Do It: Let's achieve performance greater than 98%!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        self.conv0 = nn.Conv2d(1, 8, 3, 1, 1)\n",
    "        self.conv0_bn = nn.BatchNorm2d(8)\n",
    "        self.pool0 = nn.MaxPool2d(2)\n",
    "        self.dropout0 = nn.Dropout(0.25)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(8, 16, 3, 1, 1)\n",
    "        self.conv1_bn = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        \n",
    "        self.fc = nn.Linear(16*7*7, 128)\n",
    "        self.fc1 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv0(x)\n",
    "        x = self.conv0_bn(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool0(x)\n",
    "        x = self.dropout0(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv1_bn(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
    "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
    "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   500] loss: 0.416\n",
      "[1,  1000] loss: 0.183\n",
      "[1,  1500] loss: 0.150\n",
      "[2,   500] loss: 0.108\n",
      "[2,  1000] loss: 0.106\n",
      "[2,  1500] loss: 0.101\n",
      "[3,   500] loss: 0.079\n",
      "[3,  1000] loss: 0.086\n",
      "[3,  1500] loss: 0.080\n",
      "[4,   500] loss: 0.069\n",
      "[4,  1000] loss: 0.070\n",
      "[4,  1500] loss: 0.067\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = trainDataLoader,\n",
    "                  testloader = testDataLoader,\n",
    "                  net = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)\n",
    "\n",
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set:  Accuracy: 9881/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion matrix\n",
      "\n",
      "[[ 977    1    0    0    0    0    0    1    1    0]\n",
      " [   0 1133    0    0    0    0    1    1    0    0]\n",
      " [   1    0 1026    0    0    0    0    5    0    0]\n",
      " [   0    0    5  995    0    6    0    2    1    1]\n",
      " [   0    1    1    0  969    0    2    1    0    8]\n",
      " [   2    0    0    1    0  888    1    0    0    0]\n",
      " [   6    3    0    0    1    4  943    0    1    0]\n",
      " [   0    0    6    0    1    0    0 1018    1    2]\n",
      " [   4    0    5    1    3    1    0    5  948    7]\n",
      " [   2    5    0    0    5    5    0    7    1  984]]\n",
      "\n",
      "Accuracy per class\n",
      "\n",
      "0 th acc:  99.6938775510204\n",
      "1 th acc:  99.8237885462555\n",
      "2 th acc:  99.4186046511628\n",
      "3 th acc:  98.51485148514851\n",
      "4 th acc:  98.67617107942974\n",
      "5 th acc:  99.55156950672645\n",
      "6 th acc:  98.43423799582463\n",
      "7 th acc:  99.0272373540856\n",
      "8 th acc:  97.3305954825462\n",
      "9 th acc:  97.52229930624381\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.compute_conf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103066"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(mnist_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q10) What are the problems with the large number of parameters?\n",
    "\n",
    "#### Ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's use Convolution Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Convolution](./imgs/Conv.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q11) Given input image with shape:(H, W, C1), what would be the shape of output image after applying 2 (F * F) convolutional filters with stride S?\n",
    "#### Ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (9) 2-Layer Network (Conv+Fc) + ReLU + Adam + Batch-Norm\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Conv: 8 (6 * 6) filter with stride=2 \n",
    "- Hidden dimension: 8 * 12 * 12\n",
    "- Output dimension: 10\n",
    "- activation: relu\n",
    "- normalization: batch-norm\n",
    "- Optimizer: Adam\n",
    "- Loss: Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        self.conv0 = nn.Conv2d(in_channels = 1,\n",
    "                               out_channels = 8,\n",
    "                               kernel_size = 6,\n",
    "                               stride = 2) # Layer 1\n",
    "        self.conv0_bn = nn.BatchNorm2d(8)  # 2d batch-norm is used in 3d inputs\n",
    "        self.fc = nn.Linear(8*12*12, 10)   # Layer 2 \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv0(x)\n",
    "        x = self.conv0_bn(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(trainloader = trainDataLoader,\n",
    "                  testloader = testDataLoader,\n",
    "                  net = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(mnist_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q12) How did the performance and the number of parameters change after using the Convolution operation? Why did these results come out?\n",
    "\n",
    "#### Ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (10) 2-Layer Network (Conv+Pool+Fc) + ReLU + Adam + Batch-Norm\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Conv: 8 (7 * 7) filter with stride=2 \n",
    "- Pool: 2 * 2\n",
    "- Hidden dimension: 8 * 6 * 6\n",
    "- Output dimension: 10\n",
    "- activation: relu\n",
    "- normalization: batch-norm\n",
    "- Optimizer: Adam\n",
    "- Loss: Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pooling](./imgs/Pool.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        self.conv0 = nn.Conv2d(in_channels = 1,\n",
    "                               out_channels = 8,\n",
    "                               kernel_size = 6,\n",
    "                               stride = 2) # Layer 1\n",
    "        self.conv0_bn = nn.BatchNorm2d(8)  \n",
    "        self.pool0 = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(8*6*6, 10) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv0(x)\n",
    "        x = self.conv0_bn(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool0(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() \n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(trainloader = trainDataLoader,\n",
    "                  testloader = testDataLoader,\n",
    "                  net = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(mnist_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q13) How did the performance change after using the Convolution operation? Why did these results come out?\n",
    "\n",
    "#### Ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
