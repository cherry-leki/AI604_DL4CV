{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright @ cb_park@korea.ac.kr (Cheonbok Park), joonleesky@kaist.ac.kr (Hojoon Lee)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Feed-Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn #\n",
    "import torch.nn.functional as F # various activation functions for model\n",
    "import torchvision # You can load various Pretrained Model from this package \n",
    "import torchvision.datasets as vision_dsets\n",
    "import torchvision.transforms as T # Transformation functions to manipulate images\n",
    "import torch.optim as optim # various optimization functions for model\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable \n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initilaize Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MNIST_DATA(root='./data',train =True,transforms=None ,download =True,batch_size = 32,num_worker = 1):\n",
    "    print (\"[+] Get the MNIST DATA\")\n",
    "    \"\"\"\n",
    "    We will use Mnist data for our tutorial \n",
    "    \"\"\"\n",
    "    mnist_train = vision_dsets.MNIST(root = root,  #root is the place to store your data. \n",
    "                                    train = True,  \n",
    "                                    transform = T.ToTensor(), # convert data to tensor \n",
    "                                    download = True)  # whether to download the data\n",
    "    mnist_test = vision_dsets.MNIST(root = root,\n",
    "                                    train = False, \n",
    "                                    transform = T.ToTensor(),\n",
    "                                    download = True)\n",
    "    \"\"\"\n",
    "    Data Loader is a iterator that fetches the data with the number of desired batch size. \n",
    "    * Practical Guide : What is the optimal batch size? \n",
    "      - Usually.., higher the batter. \n",
    "      - We recommend to use it as a multiple of 2 to efficiently utilize the gpu memory. (related to bit size)\n",
    "    \"\"\"\n",
    "    trainDataLoader = data.DataLoader(dataset = mnist_train,  # information about your data type\n",
    "                                      batch_size = batch_size, # batch size\n",
    "                                      shuffle =True, # Whether to shuffle your data for every epoch. (Very important for training performance)\n",
    "                                      num_workers = 1) # number of workers to load your data. (usually number of cpu cores)\n",
    "\n",
    "    testDataLoader = data.DataLoader(dataset = mnist_test, \n",
    "                                    batch_size = batch_size,\n",
    "                                    shuffle = False, # we don't actually need to shuffle data for test\n",
    "                                    num_workers = 1) #\n",
    "    print (\"[+] Finished loading data & Preprocessing\")\n",
    "    return mnist_train,mnist_test,trainDataLoader,testDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Get the MNIST DATA\n",
      "[+] Finished loading data & Preprocessing\n"
     ]
    }
   ],
   "source": [
    "trainDset,testDset,trainDataLoader,testDataLoader= MNIST_DATA(batch_size = 32)  # Data Loader "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, trainloader, testloader, net, optimizer, criterion):\n",
    "        \"\"\"\n",
    "        trainloader: train data's loader\n",
    "        testloader: test data's loader\n",
    "        net: model to train\n",
    "        optimizer: optimizer to update your model\n",
    "        criterion: loss function\n",
    "        \"\"\"\n",
    "        self.trainloader = trainloader\n",
    "        self.testloader = testloader\n",
    "        self.net = net\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        \n",
    "    def train(self, epoch = 1):\n",
    "        \"\"\"\n",
    "        epoch: number of times each training sample is used\n",
    "        \"\"\"\n",
    "        self.net.train()\n",
    "        for e in range(epoch):\n",
    "            running_loss = 0.0  \n",
    "            for i, data in enumerate(self.trainloader, 0): \n",
    "                # get the inputs\n",
    "                inputs, labels = data # Return type for data in dataloader is tuple of (input_data, labels)\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "                # zero the parameter gradients\n",
    "                self.optimizer.zero_grad()    \n",
    "                #  Q1) what if we dind't clear up the gradients?\n",
    "                #  A1) Whenever .background() is called, Pytorch accumulates the gradients in buffers on subsequent backward pass.\n",
    "                #      If we didn't clear up them, it can be mixed with the previous gradients, and values can diverge\n",
    "                #      when the number of iterations increases. So, if you don't want to mix up gradients between minibatches,\n",
    "                #      you have to zero them out at the start of a new minibatch.\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = self.net(inputs) # get output after passing through the network\n",
    "                loss = self.criterion(outputs, labels) # compute model's score using the loss function \n",
    "                loss.backward() # perform back-propagation from the loss\n",
    "                self.optimizer.step() # perform gradient descent with given optimizer\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "                if (i+1) % 500 == 0:    # print every 2000 mini-batches\n",
    "                    print('[%d, %5d] loss: %.3f' % (e + 1, i + 1, running_loss / 500))\n",
    "                    running_loss = 0.0\n",
    "\n",
    "        print('Finished Training')\n",
    "        \n",
    "    def test(self):\n",
    "        self.net.eval() # Q2) Why should we change the network into eval-mode?\n",
    "                        # A2) model.eval() sets the module in evaluation mode.\n",
    "                        #     It is a kind of switch for some specific layers/parts (e.g. Dropout, BatchNorm) of the model\n",
    "                        #     that behave differently during training and inference time.\n",
    "                        #     For example, in eval mode the Dropout is deactivated and BatchNorm uses the parameters saved in training.\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        error = 0\n",
    "        \n",
    "        # Data for confusion matrix\n",
    "        for inputs, labels in self.testloader:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda() \n",
    "            output = self.net(inputs) \n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max \n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "            error += (~pred.eq(labels.view_as(pred))).sum().item()\n",
    "\n",
    "            test_loss /= len(self.testloader.dataset)\n",
    "            \n",
    "                \n",
    "        print('\\nTest set:  Accuracy: {}/{} ({:.0f}%)\\n'.\n",
    "                format(correct, len(self.testloader.dataset),\n",
    "                100.* correct / len(self.testloader.dataset)))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def compute_conf(self):\n",
    "        self.net.eval()\n",
    "        \n",
    "        # Data for confusion matrix\n",
    "        conf_true = torch.zeros(0, dtype=torch.long, device='cpu')\n",
    "        conf_pred = torch.zeros(0, dtype=torch.long, device='cpu')\n",
    "\n",
    "        for inputs, labels in self.testloader:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda() \n",
    "            output = self.net(inputs) \n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max \n",
    "            \n",
    "            # Append batch prediction results for confusion matrix\n",
    "            conf_true = torch.cat([conf_true, labels.view(-1).cpu()])\n",
    "            conf_pred = torch.cat([conf_pred, pred.view(-1).cpu()])\n",
    "\n",
    "        # Print confusion matrix\n",
    "        label_name = [str(i) for i in range(10)]\n",
    "        conf_mat = confusion_matrix(conf_true.numpy(), conf_pred.numpy(), labels=[i for i in range(10)])\n",
    "        print('\\nConfusion matrix\\n')\n",
    "        print(conf_mat)\n",
    "\n",
    "        # confusion matrix figure\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        for i in range(10):\n",
    "            for j in range(10):\n",
    "                texts = ax.text(j, i, conf_mat[i, j], ha=\"center\", va=\"center\", color=\"w\")\n",
    "        cax = ax.matshow(conf_mat)\n",
    "        fig.colorbar(cax)\n",
    "        ax.set_xticks([i for i in range(10)])\n",
    "        ax.set_yticks([i for i in range(10)])\n",
    "        ax.set_xticklabels(label_name)\n",
    "        ax.set_yticklabels(label_name)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create Model by yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![activation](./imgs/activation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 2-Layer Network + Sigmoid\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Hidden dimension: 30\n",
    "- Output dimension: 10\n",
    "- activation: sigmoid\n",
    "- Optimizer: SGD\n",
    "- Loss: Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__() # Q3) Why do we need to call the constructor, nn.Module?\n",
    "                                          # A3) torch.nn.Module is the base class for all neural network modules in Pytorch.\n",
    "                                          #     If we want to create a class that holds our weights, bias, and method for the forward step,\n",
    "                                          #     then nn.Module provides a number of attributes and methods for them.\n",
    "                                          #     In order to create a custom network using Pytorch, therefore, the network need to\n",
    "                                          #     inherit nn.Module.\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc0 = nn.Linear(28*28,30)\n",
    "        self.fc1 = nn.Linear(30, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,28*28) # x.view convert the shape of tensor, (Batch_size,28,28) --> (Batch_size,28*28)\n",
    "        x = self.fc0(x) # 28*28 -> 30 \n",
    "        x = F.sigmoid(x) # Activation function \n",
    "        x = self.fc1(x)  # 30 -> 10, logit for each class\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() # create the neural network instance and load to the cuda memory.\n",
    "criterion = nn.CrossEntropyLoss() # Define Loss Function. We use Cross-Entropy loss.\n",
    "optimizer = optim.SGD(mnist_net.parameters(), lr=0.001) # optimizer receives training parameters and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(trainloader = trainDataLoader,\n",
    "                  testloader = testDataLoader,\n",
    "                  net = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leki\\Anaconda3\\envs\\assn1\\lib\\site-packages\\torch\\nn\\functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   500] loss: 2.321\n",
      "[1,  1000] loss: 2.300\n",
      "[1,  1500] loss: 2.283\n",
      "[2,   500] loss: 2.263\n",
      "[2,  1000] loss: 2.251\n",
      "[2,  1500] loss: 2.240\n",
      "[3,   500] loss: 2.219\n",
      "[3,  1000] loss: 2.208\n",
      "[3,  1500] loss: 2.194\n",
      "[4,   500] loss: 2.170\n",
      "[4,  1000] loss: 2.155\n",
      "[4,  1500] loss: 2.139\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set:  Accuracy: 5507/10000 (55%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 2-Layer Network + ReLU\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Hidden dimension: 30\n",
    "- Output dimension: 10\n",
    "- activation: relu\n",
    "- Optimizer: SGD\n",
    "- Loss: Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__() \n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc0 = nn.Linear(28*28,30)\n",
    "        self.fc1 = nn.Linear(30, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,28*28) \n",
    "        x = self.fc0(x) # 28*28 -> 30 \n",
    "        x = F.relu(x) # Activation function\n",
    "        x = self.fc1(x)  # 30 -> 10\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.SGD(mnist_net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(trainloader = trainDataLoader,\n",
    "                  testloader = testDataLoader,\n",
    "                  net = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   500] loss: 2.280\n",
      "[1,  1000] loss: 2.217\n",
      "[1,  1500] loss: 2.123\n",
      "[2,   500] loss: 1.896\n",
      "[2,  1000] loss: 1.744\n",
      "[2,  1500] loss: 1.589\n",
      "[3,   500] loss: 1.323\n",
      "[3,  1000] loss: 1.190\n",
      "[3,  1500] loss: 1.072\n",
      "[4,   500] loss: 0.918\n",
      "[4,  1000] loss: 0.863\n",
      "[4,  1500] loss: 0.802\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set:  Accuracy: 8469/10000 (85%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4) Is there any difference in performance according to the activiation function?\n",
    "\n",
    "#### Ans) Yes. When comparing inference of ReLU and Sigmoid in the same Network, the accuracy of ReLU is about 85%, which is better than 59% of Sigmoid. Sigmoid function has a problem in that when the output approaches 1 or 0 (saturated), the gradient approaches 0. Then, the gradients can't backpropagate through the network. The ReLU function is not saturated in positive region, so there is no vanishing gradient problem in positive region. It is also said that the convergence speed is about 6 times faster than that of the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 3-Layer Network + Sigmoid\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Hidden dimension: (50, 30)\n",
    "- Output dimension: 10\n",
    "- activation: sigmoid\n",
    "- Optimizer: SGD\n",
    "- Loss: Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        self.fc0 = nn.Linear(28*28,50) # Layer 1\n",
    "        self.fc1 = nn.Linear(50, 30) # Layer 2\n",
    "        self.fc2 = nn.Linear(30, 10) # Layer 3\n",
    "\n",
    "    def forward(self, x):\n",
    "      \n",
    "        x = x.view(-1,28*28)\n",
    "        x = self.fc0(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.SGD(mnist_net.parameters(), lr=0.001)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(trainloader = trainDataLoader,\n",
    "                  testloader = testDataLoader,\n",
    "                  net = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   500] loss: 2.317\n",
      "[1,  1000] loss: 2.307\n",
      "[1,  1500] loss: 2.303\n",
      "[2,   500] loss: 2.301\n",
      "[2,  1000] loss: 2.301\n",
      "[2,  1500] loss: 2.300\n",
      "[3,   500] loss: 2.299\n",
      "[3,  1000] loss: 2.299\n",
      "[3,  1500] loss: 2.299\n",
      "[4,   500] loss: 2.298\n",
      "[4,  1000] loss: 2.298\n",
      "[4,  1500] loss: 2.298\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set:  Accuracy: 1135/10000 (11%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5) Is training gets done easily? If it doesn't, why not?\n",
    "\n",
    "#### Ans) No, it isn't. As mentioned in the answer of Q5, when using the sigmoid function saturated neuron kills the gradient. As the layer becomes deeper, therefore, layers at the front may not be updated due to the problem of vanishing gradient during backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) 3-Layer Network + ReLU\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Hidden dimension: (50, 30)\n",
    "- Output dimension: 10\n",
    "- activation: relu\n",
    "- Optimizer: SGD\n",
    "- Loss: Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        self.fc0 = nn.Linear(28*28,50) # Layer 1\n",
    "        self.fc1 = nn.Linear(50, 30) # Layer 2\n",
    "        self.fc2 = nn.Linear(30, 10) # Layer 3\n",
    "\n",
    "    def forward(self, x):\n",
    "      \n",
    "        x = x.view(-1,28*28)\n",
    "        x = self.fc0(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.SGD(mnist_net.parameters(), lr=0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(trainloader = trainDataLoader,\n",
    "                  testloader = testDataLoader,\n",
    "                  net = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   500] loss: 2.300\n",
      "[1,  1000] loss: 2.283\n",
      "[1,  1500] loss: 2.264\n",
      "[2,   500] loss: 2.212\n",
      "[2,  1000] loss: 2.167\n",
      "[2,  1500] loss: 2.111\n",
      "[3,   500] loss: 1.965\n",
      "[3,  1000] loss: 1.856\n",
      "[3,  1500] loss: 1.737\n",
      "[4,   500] loss: 1.519\n",
      "[4,  1000] loss: 1.398\n",
      "[4,  1500] loss: 1.281\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set:  Accuracy: 7471/10000 (75%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6) Is training gets done easily compared to experiment (2)? If it doesn't, why not?\n",
    "\n",
    "#### Ans) Yes. As mentioned in the answer of Q5, when using the ReLU function neurons are not saturated in positive region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q7) What would happen if there is no activation function?\n",
    "\n",
    "#### Ans) If there is no activation function, the output signal would simply be a single linear function and it would be equal to linear classifier. Linear equation is easy to solve but it is limited in its complexity and hard to learn complex functional mapping from data. Introducing non-linear activation functions allows for the network to solve a larger variety of problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Change our Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Adam](./imgs/adam.jpeg)\n",
    "\n",
    "Reference: 하용호, 자습해도 모르겠던 딥러닝, 머리속에 인스톨 시켜드립니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) 3-Layer Network + ReLU + Adam\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Hidden dimension: (50, 30)\n",
    "- Output dimension: 10\n",
    "- activation: relu\n",
    "- Optimizer: Adam\n",
    "- Loss: Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        self.fc0 = nn.Linear(28*28,50) # Layer 1\n",
    "        self.fc1 = nn.Linear(50, 30) # Layer 2\n",
    "        self.fc2 = nn.Linear(30, 10) # Layer 3\n",
    "\n",
    "    def forward(self, x):\n",
    "      \n",
    "        x = x.view(-1,28*28)\n",
    "        x = self.fc0(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() \n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(trainloader = trainDataLoader,\n",
    "                  testloader = testDataLoader,\n",
    "                  net = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   500] loss: 0.620\n",
      "[1,  1000] loss: 0.300\n",
      "[1,  1500] loss: 0.230\n",
      "[2,   500] loss: 0.190\n",
      "[2,  1000] loss: 0.163\n",
      "[2,  1500] loss: 0.150\n",
      "[3,   500] loss: 0.125\n",
      "[3,  1000] loss: 0.117\n",
      "[3,  1500] loss: 0.121\n",
      "[4,   500] loss: 0.097\n",
      "[4,  1000] loss: 0.090\n",
      "[4,  1500] loss: 0.101\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set:  Accuracy: 9663/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) 2-Layer Network + ReLU + Adam\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Hidden dimension: (30)\n",
    "- Output dimension: 10\n",
    "- activation: relu\n",
    "- Optimizer: Adam\n",
    "- Loss: Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__() \n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc0 = nn.Linear(28*28,30)\n",
    "        self.fc1 = nn.Linear(30, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,28*28) \n",
    "        x = self.fc0(x) # 28*28 -> 30 \n",
    "        x = F.relu(x) \n",
    "        x = self.fc1(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(trainloader = trainDataLoader,\n",
    "                  testloader = testDataLoader,\n",
    "                  net = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   500] loss: 0.690\n",
      "[1,  1000] loss: 0.335\n",
      "[1,  1500] loss: 0.292\n",
      "[2,   500] loss: 0.246\n",
      "[2,  1000] loss: 0.228\n",
      "[2,  1500] loss: 0.210\n",
      "[3,   500] loss: 0.193\n",
      "[3,  1000] loss: 0.183\n",
      "[3,  1500] loss: 0.164\n",
      "[4,   500] loss: 0.149\n",
      "[4,  1000] loss: 0.159\n",
      "[4,  1500] loss: 0.154\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set:  Accuracy: 9558/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch-Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![normalization](./imgs/normalization.png)\n",
    "\n",
    "Reference: Andrew Ng, Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7) 2-Layer Network + ReLU + Adam + Batch-Norm\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Hidden dimension: (30)\n",
    "- Output dimension: 10\n",
    "- activation: relu\n",
    "- normalization: batch-norm\n",
    "- Optimizer: Adam\n",
    "- Loss: Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__() \n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc0 = nn.Linear(28*28,30)\n",
    "        self.bn0 = nn.BatchNorm1d(30) # BatchNorm \n",
    "        self.fc1 = nn.Linear(30, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,28*28) \n",
    "        x = self.fc0(x) # 28*28 -> 30 \n",
    "        x = self.bn0(x)\n",
    "        x = F.relu(x) \n",
    "        x = self.fc1(x)   \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(trainloader = trainDataLoader,\n",
    "                  testloader = testDataLoader,\n",
    "                  net = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   500] loss: 0.728\n",
      "[1,  1000] loss: 0.334\n",
      "[1,  1500] loss: 0.293\n",
      "[2,   500] loss: 0.221\n",
      "[2,  1000] loss: 0.209\n",
      "[2,  1500] loss: 0.198\n",
      "[3,   500] loss: 0.175\n",
      "[3,  1000] loss: 0.170\n",
      "[3,  1500] loss: 0.162\n",
      "[4,   500] loss: 0.148\n",
      "[4,  1000] loss: 0.149\n",
      "[4,  1500] loss: 0.145\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set:  Accuracy: 9648/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (8) 3-Layer Network + ReLU + Adam + Batch-Norm\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Hidden dimension: (50, 30)\n",
    "- Output dimension: 10\n",
    "- activation: relu\n",
    "- normalization: batch-norm\n",
    "- Optimizer: Adam\n",
    "- Loss: Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        self.fc0 = nn.Linear(28*28,50) # Layer 1\n",
    "        self.bn0 = nn.BatchNorm1d(50) # BatchNorm 1 \n",
    "        self.fc1 = nn.Linear(50, 30) # Layer 2\n",
    "        self.bn1 = nn.BatchNorm1d(30) # BatchNorm 2\n",
    "        self.fc2 = nn.Linear(30, 10) # Layer 3\n",
    "\n",
    "    def forward(self, x):\n",
    "      \n",
    "        x = x.view(-1,28*28)\n",
    "        x = self.fc0(x)\n",
    "        x = self.bn0(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(trainloader = trainDataLoader,\n",
    "                  testloader = testDataLoader,\n",
    "                  net = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   500] loss: 0.653\n",
      "[1,  1000] loss: 0.261\n",
      "[1,  1500] loss: 0.208\n",
      "[2,   500] loss: 0.152\n",
      "[2,  1000] loss: 0.143\n",
      "[2,  1500] loss: 0.134\n",
      "[3,   500] loss: 0.110\n",
      "[3,  1000] loss: 0.119\n",
      "[3,  1500] loss: 0.121\n",
      "[4,   500] loss: 0.096\n",
      "[4,  1000] loss: 0.099\n",
      "[4,  1500] loss: 0.104\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set:  Accuracy: 9731/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q8) Is there any performance difference before/after applying the batch-norm?\n",
    "\n",
    "#### Ans) A little bit. In the case of 2-layer network (comparing (7) with (6)), the performance of both is similar at 96%. In the case of a 3-layer network (comparing (8) with (5)), there is a 1% improvement in the performance from 97% to 98%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q9) How did 2-layer neural network and 3-layer neural network behave differently after applying the batch-nrom?\n",
    "\n",
    "#### Ans) Comparing (8) with (7), there is a 2% improvement in the performance from 96% to 98%. As the number of layers increases, a vanishing gradient problem in which the gradient is not transmitted during backpropagation may occur. For this, there is a batch normalization technique that normalizes the distribution of batches in each layer. As the layer becomes deeper, the probability of occurrence of vanishing gradient problem increases, so the effect of batch normalization increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 1.1 Let's Do It: Let's achieve performance greater than 98%!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        self.conv0 = nn.Conv2d(1, 8, 3, 1, 1)\n",
    "        self.conv0_bn = nn.BatchNorm2d(8)\n",
    "        self.pool0 = nn.MaxPool2d(2)\n",
    "        self.dropout0 = nn.Dropout(0.25)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(8, 16, 3, 1, 1)\n",
    "        self.conv1_bn = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        \n",
    "        self.fc = nn.Linear(16*7*7, 128)\n",
    "        self.fc1 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv0(x)\n",
    "        x = self.conv0_bn(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool0(x)\n",
    "        x = self.dropout0(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv1_bn(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
    "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
    "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   500] loss: 0.443\n",
      "[1,  1000] loss: 0.191\n",
      "[1,  1500] loss: 0.148\n",
      "[2,   500] loss: 0.107\n",
      "[2,  1000] loss: 0.102\n",
      "[2,  1500] loss: 0.104\n",
      "[3,   500] loss: 0.086\n",
      "[3,  1000] loss: 0.078\n",
      "[3,  1500] loss: 0.077\n",
      "[4,   500] loss: 0.070\n",
      "[4,  1000] loss: 0.065\n",
      "[4,  1500] loss: 0.067\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = trainDataLoader,\n",
    "                  testloader = testDataLoader,\n",
    "                  net = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)\n",
    "\n",
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set:  Accuracy: 9897/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion matrix\n",
      "\n",
      "[[ 977    0    0    0    0    0    2    1    0    0]\n",
      " [   0 1124    2    1    0    0    2    4    2    0]\n",
      " [   1    0 1020    1    0    0    0    8    2    0]\n",
      " [   0    0    1 1002    0    3    0    1    3    0]\n",
      " [   0    0    0    0  969    0    3    1    1    8]\n",
      " [   3    0    0    2    0  882    4    0    1    0]\n",
      " [   4    2    0    0    1    2  948    0    1    0]\n",
      " [   1    0    2    1    0    0    0 1021    2    1]\n",
      " [   2    0    1    1    1    1    0    0  965    3]\n",
      " [   0    0    0    1    6    5    0    4    4  989]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.compute_conf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103066"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(mnist_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q10) What are the problems with the large number of parameters?\n",
    "\n",
    "#### Ans) Network that uses many parameters can represent more complicated functions because more can be expressed. Therefore, normally the more parameters, the better to the network. As the network gets bigger, however, the memory usage increases and . So, we need to adjust the network size to suit our resources. Also, overfitting can occur as the number of parameters increases. In this case, regularization should be strongly applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's use Convolution Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Convolution](./imgs/Conv.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q11) Given input image with shape:(H, W, C1), what would be the shape of output image after applying 2 (F * F) convolutional filters with stride S?\n",
    "#### Ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (9) 2-Layer Network (Conv+Fc) + ReLU + Adam + Batch-Norm\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Conv: 8 (6 * 6) filter with stride=2 \n",
    "- Hidden dimension: 8 * 12 * 12\n",
    "- Output dimension: 10\n",
    "- activation: relu\n",
    "- normalization: batch-norm\n",
    "- Optimizer: Adam\n",
    "- Loss: Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        self.conv0 = nn.Conv2d(in_channels = 1,\n",
    "                               out_channels = 8,\n",
    "                               kernel_size = 6,\n",
    "                               stride = 2) # Layer 1\n",
    "        self.conv0_bn = nn.BatchNorm2d(8)  # 2d batch-norm is used in 3d inputs\n",
    "        self.fc = nn.Linear(8*12*12, 10)   # Layer 2 \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv0(x)\n",
    "        x = self.conv0_bn(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(trainloader = trainDataLoader,\n",
    "                  testloader = testDataLoader,\n",
    "                  net = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   500] loss: 0.387\n",
      "[1,  1000] loss: 0.170\n",
      "[1,  1500] loss: 0.132\n",
      "[2,   500] loss: 0.096\n",
      "[2,  1000] loss: 0.085\n",
      "[2,  1500] loss: 0.085\n",
      "[3,   500] loss: 0.068\n",
      "[3,  1000] loss: 0.065\n",
      "[3,  1500] loss: 0.064\n",
      "[4,   500] loss: 0.056\n",
      "[4,  1000] loss: 0.055\n",
      "[4,  1500] loss: 0.056\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set:  Accuracy: 9826/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11842"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(mnist_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q12) How did the performance and the number of parameters change after using the Convolution operation? Why did these results come out?\n",
    "\n",
    "#### Ans) Comparing (9) with (7), the performance improved by 2% (96% -> 98%) and the number of parameters decreased by about 2 times. (23,920 -> 11,842) In the linear layer, as the size of the input increases, the number of weights used by the hidden layer increases. This can cause the problems mentioned in Q10. When the convolution layer is used, parameters are used as much as the size and the number of filters, so the number of parameters can be reduced while extracting features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (10) 2-Layer Network (Conv+Pool+Fc) + ReLU + Adam + Batch-Norm\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Conv: 8 (7 * 7) filter with stride=2 \n",
    "- Pool: 2 * 2\n",
    "- Hidden dimension: 8 * 6 * 6\n",
    "- Output dimension: 10\n",
    "- activation: relu\n",
    "- normalization: batch-norm\n",
    "- Optimizer: Adam\n",
    "- Loss: Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pooling](./imgs/Pool.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        self.conv0 = nn.Conv2d(in_channels = 1,\n",
    "                               out_channels = 8,\n",
    "                               kernel_size = 6,\n",
    "                               stride = 2) # Layer 1\n",
    "        self.conv0_bn = nn.BatchNorm2d(8)  \n",
    "        self.pool0 = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(8*6*6, 10) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv0(x)\n",
    "        x = self.conv0_bn(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool0(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() \n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(trainloader = trainDataLoader,\n",
    "                  testloader = testDataLoader,\n",
    "                  net = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   500] loss: 0.554\n",
      "[1,  1000] loss: 0.196\n",
      "[1,  1500] loss: 0.155\n",
      "[2,   500] loss: 0.114\n",
      "[2,  1000] loss: 0.104\n",
      "[2,  1500] loss: 0.100\n",
      "[3,   500] loss: 0.089\n",
      "[3,  1000] loss: 0.084\n",
      "[3,  1500] loss: 0.082\n",
      "[4,   500] loss: 0.077\n",
      "[4,  1000] loss: 0.076\n",
      "[4,  1500] loss: 0.075\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set:  Accuracy: 9771/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3202"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(mnist_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q13) How did the performance change after using the Convolution operation? Why did these results come out?\n",
    "\n",
    "#### Ans) Comparing (10) with (9), the performance is similar, but the number of parameters is significantly reduced. (11842 -> 3202) When using a 2D pooling layer, the number of channels is maintained, and the width and height of the input are reduced by extracting the features of each part of the input. This reduces the number of parameters of the network to suppress overfitting problem and reduce memory usage. Also, since the pooling layer has no parameters to be trained, it does not affect the total number of parameters of the network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
